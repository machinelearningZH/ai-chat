# LLM Model Configuration
model:
  default_selection: "llama3.2:1B"
  models:
    # https://docs.unsloth.ai/models/qwen3-how-to-run-and-fine-tune/qwen3-2507
    - name: "llama3.2:1B"
      temperature: 0.7
      max_tokens_context: 3000 # see README how to increase context length
      max_tokens_output: 1024
    # https://docs.unsloth.ai/models/qwen3-how-to-run-and-fine-tune/qwen3-2507
    - name: "hf.co/unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF:Q6_K"
      temperature: 0.7
      max_tokens_context: 65536 # see README how to increase context length
      max_tokens_output: 16192
    # https://docs.unsloth.ai/models/qwen3-coder-how-to-run-locally
    - name: "hf.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF:q6_k"
      temperature: 0.7
      max_tokens_context: 256000 # see README how to increase context length
      max_tokens_output: 16192

# OpenAI API Configuration for Ollama
openai:
  base_url: "http://localhost:11434/v1"
  api_key: "ollama" # Dummy value, Ollama does not require an actual API key

# Chat Configuration
chat:
  default_mode: true
  app_name: "AI Chat"

# Logging Configuration
logging:
  log_file: "chainlit_logs.log"
  log_level: "INFO"

# Defaults
default_ollama_max_tokens: 4096
default_max_tokens_output: 1024
default_temperature: 0.7

# Decrease max_tokens_context by this value to account for prompt overhead and unprecise token counting.
# We use tiktokens default encoding for estimation, which might differ from the actual model encoding.
context_token_buffer: 2048

# File format whitelist for document uploads
file_format_whitelist:
  - ".txt"
  - ".py"
  - ".md"
  - ".Rmd"
  - ".rmd"
  - ".r"
  - ".R"
  - ".bin"
